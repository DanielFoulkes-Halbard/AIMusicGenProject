Project weekly log

Week 1:

•	Choose project and supervisor.
•	Choose Google Colab as main Python editor.
•	Researched various Python libraries that can deal with MIDI files.
•	Discovered MIDI files can be split into individual tracks, but track instruments are rarely labelled. Might be hard to distinguish between rhythm/bass/drum sections.
•	Had a play with the Lahk data set. Discovered tracks are often in strange file configurations. Might be useful to combine or use different data sets for different purposes.

Week 2:

•	Started some reading on various music generation techniques. Found a good paper that outlines different approaches.
•	Looked into other datasets apart from Lahk such as googleAI magenta.
•	Started weekly log.
•	Started project proposal.
•	Set up calendar for weekly work rota.
•	Finished first draft of project proposal.
•	Read A LOT of research papers.

Week3:

•	Submitted Project proposal to Kingsley via email
•	Started uploading data sets to google drive for colab use
•	Sourcing different data sets to use with model:lahk, bitmidi, ADL piano midi and classical archives
•	ADL piano is a subset based off lahk only using the “piano” parts so would be good for rhyme maybe
•	Had meeting with Kingsley he mentioned Pre processing data: Relative pitch instead of absolute pitch, filter (normalise key). Both all happy so far. Mentioned getting a single track prototype in the works for week 7 interim report.
•	Decided o focus on classical music
•	Made ADL_classic dataset mainly from ADL piano(1160) with small amounts from classical archive(5) and bitmidi(40). Combined them via python colab and backed them up. Some files in ADL piano would not load due to weird sharp properties.
•	Thoughts on how to approach development:
- Generate single track first which satisfies structure, rhythm dynamics etc and sequentially add tracks to original somehow.
- Generate all tracks at once through a more complicated model.
- Hybrid approach. Generate initial single track then use multi track model that uses the initial track to generate additional tracks.
Week 4:
•	Started initial work on attention RNN for single track MIDI output
•	Looking into OpenAI Musenet and sparse transformers
•	Decided to try and play with my own RNN arctitecture based on attentive RNNs
•	Pre-processed data in [track, note sequence] format
•	Did sequence padding to the average length of data for uniform inputs
•	First steps in defining the RNN model (LTSM)
Week 5:
•	Realised I might be running before I can walk…. Decided to try a simpler approach first (simple multi layer LTSM regression task might not work). And will document my improvements to the model and data pre processing as I go and learn more. Eventually ending up with an attention mechanism.
•	Various experiments trying to get to grips with MIDI files and potential pythons framworks I can use. Found great tutorial on Kaggle.
•	Tried very hard to get tensorflow to work with local gpu but no luck. Might have to use colab plus
•	Started interim report
•	Researched tokenization methods more thoroughly 

Week 6:
•	Continued writing up interim report
•	Explored other methods of processing
•	Tried to get home PC set up with graphics card to run models
•	Finished interim first draft
•	Sent first draft to supervisor for feedback
